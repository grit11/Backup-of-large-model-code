# 创建 conda 环境
conda create -n vllm_env python=3.12 -y
conda activate vllm_env

# 安装 vLLM + FlashAttention
pip install "vllm[all]"
pip install flash-attn --no-build-isolation

vLLM（https://github.com/vllm-project/vllm）是一个高性能大语言模型推理引擎，核心目标是极大提升大模型部署时的吞吐量和多并发能力，尤其适合服务端API场景。
FlashAttention（https://github.com/Dao-AILab/flash-attention）是当前最主流的高性能**Transformer注意力加速库**。它对“注意力计算”部分进行了底层优化，提升了速度同时**大幅降低显存占用**。
vLLM 内部集成/兼容 FlashAttention，二者配合能让大模型的推理速度和资源利用达到当前开源体系的“天花板”级别。

#安装modelscope库(从该库下载模型)
pip install modelscope


#千问2.5-VL-72B-Instruct模型运行环境
72B (720亿参数) 巨型模型
显存：》=160G，显卡：2xA100(160G显存) 4xL20（192G）需多卡并行启动
数据盘：模型权重占用数据盘140G，预留300G以上空间存放图像、数据集、日志等
镜像选择： 在AutoDL创建实例时，选择预装较新PyTorch、CUDA和基础Python环境的镜像（如PyTorch 2.0.1，CUDA 11.8或更高版本镜像）
